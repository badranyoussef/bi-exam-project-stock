{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have created three CSV files, in which our data are split. Because we are missing a larger portion of data from before 1987 and after 2024, we have split our data in three files:\n",
    "1. Containing data from before 1987\n",
    "2. Containing data from 1987 to 2017\n",
    "3. Containing data from 1987 to 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new combined CSV's with data\n",
    "\n",
    "df_before_1987 = pd.read_csv('') #insert\n",
    "df_1987_2017 = pd.read_csv('') #insert PRIMARY\n",
    "df_1987_2024 = pd.read_csv('') #insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1987_2017.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "Let's see how the data looks in a line plot for the data frame df_1987_2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1987_2017.plot.line(y=['Interest Rate', 'Inflation Rate', 'Unemployment Rate', 'Volume SP500' 'Close SP500', 'Close Gold', 'Volume RUSSELL2000' 'Close RUSSELL2000', 'Close Oil', 'CPIAUCSL'] , x='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have big diffences in the our graphs, and high numbers for some data makes other data unreadable in this plot. That suggest we might have to normalize our data at some point to make et more comparable.\n",
    "\n",
    "## Normalization\n",
    "We want to normalize our data to be able to better compare it, and then see in a line plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "date_column = df_1987_2017['Date']\n",
    "# Use scaler on df\n",
    "df_1987_2017_scaled = scaler.fit_transform(df_1987_2017.drop(columns=['Date']))\n",
    "\n",
    "# Convert back to a data frame\n",
    "df_1987_2017_scaled = pd.DataFrame(df_1987_2017_scaled, columns=df_1987_2017.columns)\n",
    "\n",
    "# Add the date column back by concatinating\n",
    "df_1987_2017_scaled = pd.concat([date_column, df_1987_2017_scaled], axis=1)\n",
    "\n",
    "df_1987_2017_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of the scaled data set\n",
    "df_1987_2017_scaled.plot.line(y=['Interest Rate', 'Inflation Rate', 'Unemployment Rate', 'Volume SP500' 'Close SP500', 'Close Gold', 'Volume RUSSELL2000' 'Close RUSSELL2000', 'Close Oil', 'CPIAUCSL'] , x='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution\n",
    "Now let's have a look at histograms for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to check\n",
    "columns_to_check = df_1987_2017.drop(columns=['Date'])\n",
    "\n",
    "# Plot histogrammer for hver kolonne\n",
    "for col in columns_to_check:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(df_1987_2017[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment something about the histograms\n",
    "\n",
    "### Outliers\n",
    "Now let's have a look at outliers in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_to_check:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df_1987_2017.boxplot(column=[col])\n",
    "    plt.title(f'Box plot for {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment something about the outliers\n",
    "\n",
    "## Data correlation\n",
    "Let's take a look of the initial correlation of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "corrmatt_1987_2017 = df_1987_2017[['Interest Rate', 'Inflation Rate', 'Unemployment Rate', 'Volume SP500' 'Close SP500', 'Close Gold', 'Volume RUSSELL2000' 'Close RUSSELL2000', 'Close Oil', 'CPIAUCSL']].corr()\n",
    "sns.heatmap(corrmatt_1987_2017, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation (1987-2017)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering - adding column with change values\n",
    "We want to add column with the changes in Open and Close, and Low and High prices for our indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the Open and Close value on the same day\n",
    "df_1987_2017['OPEN_CLOSE_CHANGE_%_SP500'] = (df_1987_2017['Close SP500'] - df_1987_2017['Open SP500']) / df_1987_2017['Open SP500']\n",
    "df_1987_2017['OPEN_CLOSE_CHANGE_%_RUSSELL2000'] = (df_1987_2017['Close RUSSELL2000'] - df_1987_2017['Open RUSSELL2000']) / df_1987_2017['Open RUSSELL2000']\n",
    "df_1987_2017['OPEN_CLOSE_CHANGE_%_Gold'] = (df_1987_2017['Close Gold'] - df_1987_2017['Open Gold']) / df_1987_2017['Open Gold']\n",
    "# Calculate the difference between the Low and High value on the same day\n",
    "df_1987_2017['LOW_HIGH_CHANGE_%_SP500'] = (df_1987_2017['High SP500'] - df_1987_2017['Low SP500']) / df_1987_2017['Low SP500']\n",
    "df_1987_2017['LOW_HIGH_CHANGE_%_RUSSELL2000'] = (df_1987_2017['High RUSSELL2000'] - df_1987_2017['Low RUSSELL2000']) / df_1987_2017['Low RUSSELL2000']\n",
    "df_1987_2017['LOW_HIGH_CHANGE_%_Gold'] = (df_1987_2017['High Gold'] - df_1987_2017['Low Gold']) / df_1987_2017['Low Gold']\n",
    "\n",
    "# Calculate other changes PERCENT OR ABS??????\n",
    "df_1987_2017['Interest_Rate_Change'] = df_1987_2017['Interest Rate'].diff()\n",
    "df_1987_2017['Inflation_Rate_Change'] = df_1987_2017['Inflation Rate'].diff()\n",
    "df_1987_2017['Unemployment_Rate_Change'] = df_1987_2017['Unemployment Rate'].diff()\n",
    "df_1987_2017['CPI_Change'] = df_1987_2017['CPI'].diff()\n",
    "\n",
    "# Volume changes\n",
    "df_1987_2017['VOLUME_CHANGE_%_RUSSELL2000'] = df_1987_2017['Volume RUSSELL2000'].pct_change()\n",
    "df_1987_2017['VOLUME_CHANGE_%_SP500'] = df_1987_2017['Volume SP500'].pct_change()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
